{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\neeraj.saini\\AppData\\Local\\anaconda3\\envs\\py38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import to_numpy\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2-small\"\n",
    "model = HookedTransformer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get activation for a particular neuron.\n",
    "def get_neuron_acts(text, layer, neuron_index):\n",
    "    # Hacky way to get out state from a single hook - we have a single element list and edit that list within the hook.\n",
    "    cache = {}\n",
    "\n",
    "    def caching_hook(act, hook):\n",
    "        cache[\"activation\"] = act[0, :, neuron_index]\n",
    "\n",
    "    model.run_with_hooks(\n",
    "        text, fwd_hooks=[(f\"blocks.{layer}.mlp.hook_post\", caching_hook)]\n",
    "    )\n",
    "    return to_numpy(cache[\"activation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduced time form 60 min to some seconds.\n",
    "def get_neuron_acts_layer(text, layer):\n",
    "    # getting activation one layer at a time\n",
    "    # Return activation corresponding to each neuron in a layer having dimension eaual to number of tokens\n",
    "    cache = {}\n",
    "    activation = []\n",
    "\n",
    "    def caching_hook(act, hook):\n",
    "        cache[\"act\"] = act\n",
    "        temp = []\n",
    "        for j in range(len(cache[\"act\"][0][0])):\n",
    "            temp.append(cache[\"act\"][0,:,j].tolist())\n",
    "        # temp = np.array(temp).squeeze()\n",
    "        # print(temp.shape)\n",
    "        activation.append(temp)\n",
    "        \n",
    "\n",
    "    model.run_with_hooks(\n",
    "        text, fwd_hooks=[(f\"blocks.{layer}.mlp.hook_post\", caching_hook)]\n",
    "    )\n",
    "    return to_numpy(activation)\n",
    "\n",
    "# cache = {}\n",
    "# text = \"hunting for life\"\n",
    "# layer = 9\n",
    "# def hook_fun(act, hook):\n",
    "#     cache[\"act\"] = act\n",
    "# model.run_with_hooks(text, fwd_hooks=[(f\"blocks.{layer}.mlp.hook_post\", hook_fun)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', 'The', ' following', ' is', ' a', ' list', ' of', ' powers', ' of', ' 10', ':', ' 1', ',', ' 10', ',', ' 100', ',', ' 1000', ',', ' 10000', ',', ' 100', '000', ',', ' 100', '0000', ',', ' 100', '00000']\n",
      "[-0.08643499 -0.14071973 -0.10398154 -0.12390732 -0.04058984 -0.11064889\n",
      " -0.05189846 -0.11276118 -0.06905466 -0.11189386 -0.03059199 -0.10336889\n",
      " -0.04322361  1.5935549  -0.14205764  2.5116603  -0.1331642   2.5196698\n",
      " -0.11360838  3.0765214  -0.1163745   0.53938794  2.3499637  -0.14952153\n",
      " -0.16476357  1.9449059  -0.13690163 -0.0880248   2.1848853 ]\n"
     ]
    }
   ],
   "source": [
    "default_layer = 9\n",
    "default_neuron_index = 652\n",
    "default_text = \"The following is a list of powers of 10: 1, 10, 100, 1000, 10000, 100000, 1000000, 10000000\"\n",
    "print(model.to_str_tokens(default_text))\n",
    "print(get_neuron_acts(default_text, default_layer, default_neuron_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', 'i', ' will', ' come', ' and', ' kill', ' all', ' of', ' you']\n"
     ]
    }
   ],
   "source": [
    "default_text = 'i will come and kill all of you'\n",
    "print(model.to_str_tokens(default_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_layer = 9\n",
    "default_neuron_index = 652\n",
    "temp = get_neuron_acts(default_text, default_layer, default_neuron_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', 'She', ' promised', ' to', ' come', ' and', ' kill', ' the', ' boredom']\n"
     ]
    }
   ],
   "source": [
    "print(model.to_str_tokens('She promised to come and kill the boredom'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0 done\n",
      "layer 1 done\n",
      "layer 2 done\n",
      "layer 3 done\n",
      "layer 4 done\n",
      "layer 5 done\n",
      "layer 6 done\n",
      "layer 7 done\n",
      "layer 8 done\n",
      "layer 9 done\n",
      "layer 10 done\n",
      "layer 11 done\n"
     ]
    }
   ],
   "source": [
    "sentences = ['i will come and kill all of you', 'who will you kill here an now today', 'I must kill the urge to come back.', 'I would kill to be in your shoes', \"Don't let them come and kill your spirit\", 'She promised to come and kill the boredom']\n",
    "\n",
    "default_text = \"She promised to come and kill the boredom\"\n",
    "\n",
    "act5 = []\n",
    "\n",
    "for i in range(12):\n",
    "    default_layer = i\n",
    "    ls = []\n",
    "    for j in range(3072):\n",
    "        default_neuron_index = j\n",
    "        temp = get_neuron_acts(default_text, default_layer, default_neuron_index)\n",
    "        ls.append(temp[6])\n",
    "    act5.append(ls)\n",
    "    print(f'layer {i} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0 done\n",
      "layer 1 done\n",
      "layer 2 done\n",
      "layer 3 done\n",
      "layer 4 done\n",
      "layer 5 done\n",
      "layer 6 done\n",
      "layer 7 done\n",
      "layer 8 done\n",
      "layer 9 done\n",
      "layer 10 done\n",
      "layer 11 done\n"
     ]
    }
   ],
   "source": [
    "default_text = 'i will come and kill all of you'\n",
    "\n",
    "demo0 = []\n",
    "\n",
    "# t = get_neuron_acts_layer(default_text, 0)\n",
    "\n",
    "for i in range(12):\n",
    "    default_layer = i\n",
    "    temp = get_neuron_acts_layer(default_text, default_layer)\n",
    "    temp = temp.squeeze()\n",
    "    demo0.append(temp)\n",
    "    print(f'layer {i} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = t.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3072, 9)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing activation in a text file\n",
    "#----------uncomment to write the activations-------------------------#\n",
    "# F = open('kill5.txt', 'w')\n",
    "\n",
    "# for sublist in act5:\n",
    "#     F.write(\",\".join([str(element) for element in sublist]) + \"\\n\"+\"\\n\"+\"\\n\")\n",
    "\n",
    "# F.close()\n",
    "\n",
    "# --------------- Reading from file-----------------------------------#\n",
    "# F = open('kill1.txt', 'r')\n",
    "\n",
    "# data = F.read()\n",
    "\n",
    "# data1 = list(data.split('\\n'+'\\n'+'\\n'))\n",
    "\n",
    "# # Splitting the read file\n",
    "# act1 = []\n",
    "\n",
    "# for i in range(len(data1)-1):\n",
    "#     act1.append(list(map(float,(data1[i].split(',')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returning all those activations which are greater then a certain threshold\n",
    "def light_up(act, thres = 1.0):\n",
    "    \n",
    "    fired_neurons = []\n",
    "    for i in range(len(act)):\n",
    "        ls = []\n",
    "        for j in range(len(act[0])):\n",
    "            if act[i][j] >= thres:\n",
    "                ls.append(act[i][j])\n",
    "            else:\n",
    "                ls.append(int(0))\n",
    "        fired_neurons.append(ls)\n",
    "\n",
    "    return fired_neurons\n",
    "\n",
    "def Neuron_index(activated_neurons):\n",
    "    return np.nonzero(activated_neurons)# Return a tuple of (nonzero_row_index, nonzero_column_index)\n",
    "\n",
    "def word_of_interest(act, index):\n",
    "    # Returning all the activations of a particular word of interest\n",
    "    woi = []\n",
    "    for i in range(len(act)):\n",
    "        ls = []\n",
    "        for j in range(len(act[0])):\n",
    "            ls.append(act[i][j][index])\n",
    "        woi.append(ls)\n",
    "    return woi \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo0_woi = word_of_interest(demo0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0,  0,  8,  9, 10, 10, 10, 11], dtype=int64), array([1152, 1528, 1253,  840,   49,  689, 1793, 2910], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "demo0_activated = light_up(demo0_woi, thres=3.0)\n",
    "demo0_nonzero = Neuron_index(demo0_activated)\n",
    "print(demo0_nonzero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_act = []\n",
    "for i in range(len(sentences)):\n",
    "    default_text = sentences[i]\n",
    "    act_sentence = []\n",
    "    for j in range(12):\n",
    "        default_layer = j\n",
    "        temp = get_neuron_acts_layer(default_text, default_layer)\n",
    "        temp = temp.squeeze()\n",
    "        act_sentence.append(temp)\n",
    "        # print(f'layer {j} done')\n",
    "    print(f'sentence {i} complete')\n",
    "    all_act.append(act_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0,  0,  8,  9, 10, 10, 10, 10, 11], dtype=int64), array([1152, 1528, 1253,  840,  379,  897, 1793, 2285, 2910], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# Giving index of the word of interest manually.\n",
    "# Can automate using LIME by choosing word contributing most(k=1) and give its index\n",
    "\n",
    "temp_woi = word_of_interest(all_act[1], 4)\n",
    "temp_activated = light_up(temp_woi, thres=3.0)\n",
    "temp_nonzero = Neuron_index(temp_activated)\n",
    "print(temp_nonzero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0,  0,  8,  9, 10, 10, 10, 11], dtype=int64), array([1152, 1528, 1253,  840,   49,  689, 1793, 2910], dtype=int64))\n",
      "(array([ 0,  0,  8,  9, 10, 10, 10, 10, 11], dtype=int64), array([1152, 1528, 1253,  840,  379,  897, 1793, 2285, 2910], dtype=int64))\n",
      "(array([ 0,  0,  2,  8,  9, 10, 10, 10, 11, 11], dtype=int64), array([1152, 1528,  783, 1253,  840,  379,  689, 1793, 1611, 2910],\n",
      "      dtype=int64))\n",
      "(array([ 0,  0,  2,  8,  9, 10, 10, 11], dtype=int64), array([1152, 1528,  783, 1253,  840,  379, 1793, 2910], dtype=int64))\n",
      "(array([ 0,  8,  8,  9,  9, 10, 10, 10, 10, 10, 11, 11], dtype=int64), array([1528, 1253, 1891,  840, 1681,   49,  297,  379,  689, 1793, 1611,\n",
      "       2910], dtype=int64))\n",
      "(array([ 0,  0,  8,  9, 10, 10, 11, 11], dtype=int64), array([1152, 1528, 1253,  840,  689, 1793,  611, 2910], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(nonzero)\n",
    "print(nonzero2)\n",
    "print(nonzero1)\n",
    "print(nonzero3)\n",
    "print(nonzero4)\n",
    "print(nonzero5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "''''Not working image size is too large.\n",
    "# plt.figure(figsize = (12,3072))\n",
    "sns_plot = sns.heatmap(activated_neurons, annot=True, cmap =sns.cm.rocket_r,linecolor='white', linewidths=1)\n",
    "\n",
    "results_path = 'results.png'\n",
    "#print(results_path)\n",
    "plt.savefig(results_path, dpi=400)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This is some CSS (tells us what style )to give each token a thin gray border, to make it easy to see token separation\n",
    "# style_string = \"\"\"<style> \n",
    "#     span.token {\n",
    "#         border: 1px solid rgb(123, 123, 123)\n",
    "#         } \n",
    "#     </style>\"\"\"\n",
    "\n",
    "\n",
    "# def calculate_color(val, max_val, min_val):\n",
    "#     # Hacky code that takes in a value val in range [min_val, max_val], normalizes it to [0, 1] and returns a color which interpolates between slightly off-white and red (0 = white, 1 = red)\n",
    "#     # We return a string of the form \"rgb(240, 240, 240)\" which is a color CSS knows\n",
    "#     normalized_val = (val - min_val) / max_val\n",
    "#     return f\"rgb(240, {240*(1-normalized_val)}, {240*(1-normalized_val)})\"\n",
    "\n",
    "\n",
    "# def basic_neuron_vis(text, layer, neuron_index, max_val=None, min_val=None):\n",
    "#     \"\"\"\n",
    "#     text: The text to visualize\n",
    "#     layer: The layer index\n",
    "#     neuron_index: The neuron index\n",
    "#     max_val: The top end of our activation range, defaults to the maximum activation\n",
    "#     min_val: The top end of our activation range, defaults to the minimum activation\n",
    "\n",
    "#     Returns a string of HTML that displays the text with each token colored according to its activation\n",
    "\n",
    "#     Note: It's useful to be able to input a fixed max_val and min_val, because otherwise the colors will change as you edit the text, which is annoying.\n",
    "#     \"\"\"\n",
    "#     if layer is None:\n",
    "#         return \"Please select a Layer\"\n",
    "#     if neuron_index is None:\n",
    "#         return \"Please select a Neuron\"\n",
    "#     acts = get_neuron_acts(text, layer, neuron_index)\n",
    "#     act_max = acts.max()\n",
    "#     act_min = acts.min()\n",
    "#     # Defaults to the max and min of the activations\n",
    "#     if max_val is None:\n",
    "#         max_val = act_max\n",
    "#     if min_val is None:\n",
    "#         min_val = act_min\n",
    "#     # We want to make a list of HTML strings to concatenate into our final HTML string\n",
    "#     # We first add the style to make each token element have a nice border\n",
    "#     htmls = [style_string]\n",
    "#     # We then add some text to tell us what layer and neuron we're looking at - we're just dealing with strings and can use f-strings as normal\n",
    "#     # h4 means \"small heading\"\n",
    "#     htmls.append(f\"<h4>Layer: <b>{layer}</b>. Neuron Index: <b>{neuron_index}</b></h4>\")\n",
    "#     # We then add a line telling us the limits of our range\n",
    "#     htmls.append(\n",
    "#         f\"<h4>Max Range: <b>{max_val:.4f}</b>. Min Range: <b>{min_val:.4f}</b></h4>\"\n",
    "#     )\n",
    "#     # If we added a custom range, print a line telling us the range of our activations too.\n",
    "#     if act_max != max_val or act_min != min_val:\n",
    "#         htmls.append(\n",
    "#             f\"<h4>Custom Range Set. Max Act: <b>{act_max:.4f}</b>. Min Act: <b>{act_min:.4f}</b></h4>\"\n",
    "#         )\n",
    "#     # Convert the text to a list of tokens\n",
    "#     str_tokens = model.to_str_tokens(text)\n",
    "#     for tok, act in zip(str_tokens, acts):\n",
    "#         # A span is an HTML element that lets us style a part of a string (and remains on the same line by default)\n",
    "#         # We set the background color of the span to be the color we calculated from the activation\n",
    "#         # We set the contents of the span to be the token\n",
    "#         htmls.append(\n",
    "#             f\"<span class='token' style='background-color:{calculate_color(act, max_val, min_val)}' >{tok}</span>\"\n",
    "#         )\n",
    "\n",
    "#     return \"\".join(htmls)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "# The function outputs a string of HTML\n",
    "default_max_val = 4.0\n",
    "default_min_val = 0.0\n",
    "default_html_string = basic_neuron_vis(\n",
    "    default_text,\n",
    "    default_layer,\n",
    "    default_neuron_index,\n",
    "    max_val=default_max_val,\n",
    "    min_val=default_min_val,\n",
    ")\n",
    "\n",
    "# IPython lets us display HTML\n",
    "print(\"Displayed HTML\")\n",
    "display(HTML(default_html_string))\n",
    "\n",
    "# We can also print the string directly\n",
    "print(\"HTML String - it's just raw HTML code!\")\n",
    "print(default_html_string)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Observations:\n",
    "            How to tackle the polysementic neurons?\n",
    "            If going for a particular word then it can be used in negative and positive both the sentenes. How to differentiate between them?\n",
    "            We are going for a specific word. What if that word is divide while converting in token? For example hunting is divided in 'hun' and 'ting' tokens.\n",
    "                '''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
